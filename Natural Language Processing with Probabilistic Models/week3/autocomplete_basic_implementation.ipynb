{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Auto-complete?\n",
    "\n",
    "Sentence completion suggestions that we get when we starting our questions on Google.\n",
    "\n",
    "Developing an Auto-complete system involves creating a Language Model (LM). \n",
    "LM assigns a probability to sequence(set) of words from a reference set like Wiki pages, articles etc.\n",
    "Higher probability means higher score means the sequence would make more sense.\n",
    "\n",
    "For eg. \"I eat scrambled\", now find x such that the sentence \"I eat scrambled x\" gets highest probability. If x='eggs', the sentence will be \"I eat scrambled eggs\".\n",
    "\n",
    "Here we will use N-gram method for language modeling.\n",
    "\n",
    "N-grams are also used in Machine translation and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Our Steps:\n",
    "\n",
    "1. Load data (paragraph format)\n",
    "2. Preprocess the data:\n",
    "    a) Read the US twitter data. Every tweet is separated by \\n\n",
    "    b) Split, clean and get individual nonempty tweets\n",
    "    c) Tokenize into words and characters \n",
    "    d) Split data into train and test set (validation is not used for easy implementation)\n",
    "    d) Handling OOV words :- \n",
    "        Get word freq\n",
    "        Filter words with n+ freq\n",
    "        Replace the other words with unk, unknown token\n",
    "3. Develop n-gram Language model\n",
    "    a) Count n grams\n",
    "    b) Calculate conditional probability of next word with k-Smoothing\n",
    "4. Calculate Perplexity(PP) metric - it is like entropy. Lower PP scores have better outputs\n",
    "5. Bring every thing together and test an Auto-complete sentence\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, file):\n",
    "\n",
    "    with open(path+file,'r') as f:\n",
    "        data = f.read()[:10000] #load top 10000 characters\n",
    "    \n",
    "    return data\n",
    "\n",
    "path = '/Users/apoorvasrivastava/Documents/ML/apoorva/NLP specialization/twitterData/final/en_US/'\n",
    "file = 'en_US.twitter.txt'\n",
    "\n",
    "data = load_data(path,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. split data and strip extra spaces and remove empty tweets\n",
    "    2. tokenize each tweet into word tokens\n",
    "    \n",
    "    Input: string of text\n",
    "    output: Tokenized word sets\n",
    "    \"\"\"\n",
    "    \n",
    "    def split_tweets(data):\n",
    "\n",
    "        #split into different tweets\n",
    "        sentences = data.split('\\n')\n",
    "\n",
    "        #strip extra spaces in front and back in each tweet\n",
    "        tweets_strip = [line.strip() for line in sentences]\n",
    "\n",
    "        #remove blank tweets\n",
    "        nonempty_tweet = [tweet for tweet in tweets_strip if len(tweet)>0]\n",
    "        \n",
    "        return nonempty_tweet\n",
    "    \n",
    "        \n",
    "    def tokenize(split_tweets):\n",
    "    \n",
    "        tok_tweets = []\n",
    "        for tweet in split_tweets:\n",
    "\n",
    "            #lowercase\n",
    "            tweet_lower = tweet.lower()\n",
    "\n",
    "            #tokenize\n",
    "            tweet_tok = nltk.word_tokenize(tweet_lower)\n",
    "\n",
    "            #append\n",
    "            tok_tweets.append(tweet_tok)\n",
    "   \n",
    "        return tok_tweets\n",
    "    \n",
    "    \n",
    "    split_tweets = split_tweets(data)\n",
    "    tok_tweets = tokenize(split_tweets)\n",
    "    \n",
    "    return tok_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_data = get_tokenized_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_set(token_data):\n",
    "    \"\"\"\n",
    "    divide text string into meaningful 90%, 10% split. \n",
    "    first 90% to train, next 10% to test\n",
    "    \"\"\"\n",
    "    train_size = int(len(token_data)*0.8)\n",
    "    \n",
    "    train = token_data[:train_size]\n",
    "    test = token_data[train_size:]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_set(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling OOV words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_OOV_words(token_data,threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. calculate word frequency\n",
    "    2. create a closed vocab by only selecting words>n threshold\n",
    "    2.1 rename all the other words as <unk>\n",
    "    \n",
    "    Input: tokenized data, threshold cutoff\n",
    "    Output: closed vocabulary, all replaced tokens also\n",
    "    \"\"\"\n",
    "    \n",
    "    def word_freq(token_data):\n",
    "        \n",
    "        all_token_flat = []\n",
    "        #flatten the list\n",
    "        for element in token_data:\n",
    "            for i in element:\n",
    "                all_token_flat.append(i)\n",
    "                \n",
    "        word_freq = Counter(all_token_flat)\n",
    "    \n",
    "        return word_freq\n",
    "    \n",
    "    \n",
    "    word_freq = word_freq(token_data)\n",
    "    \n",
    "    \n",
    "    def closed_vocab(word_freq, threshold):\n",
    "        \n",
    "        #sort dict by val reversed\n",
    "        sort_word_freq = dict(sorted(word_freq.items(), key=lambda x:x[1], reverse=True))\n",
    "        \n",
    "        #select only words higher than 1 freq\n",
    "        cutoff_lower_data = dict(filter(lambda x: x[1]>1, sort_word_freq.items()))\n",
    "        \n",
    "        closed_vocab = list(cutoff_lower_data.keys())\n",
    "        \n",
    "        return closed_vocab, sort_word_freq\n",
    "    \n",
    "    \n",
    "    closed_vocab, sort_word_dict = closed_vocab(word_freq, threshold)\n",
    "    \n",
    "    \n",
    "    def replace_unknown(sort_word_dict,unknown='<unk>'):\n",
    "\n",
    "        updated_tokens=[]\n",
    "        \n",
    "        for key,val in sort_word_dict.items():\n",
    "            if val<2:\n",
    "                updated_tokens.append(unknown)\n",
    "            else:\n",
    "                updated_tokens.append(key)\n",
    "        \n",
    "        return updated_tokens\n",
    "    \n",
    "    updated_tokens = replace_unknown(word_freq)\n",
    "    \n",
    "    \n",
    "    return closed_vocab, updated_tokens\n",
    "\n",
    "\n",
    "closed_vocab,updated_tokens=hand_OOV_words(token_data,threshold=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop n-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram(sentences_token, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. for every sentence, initialize m window of size n\n",
    "    2. until m reaches m=length-n \n",
    "    3. keep appending to ngram list\n",
    "    \n",
    "    Input: List of list tokenized sentences \n",
    "    Output: list of ngrams across sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    ngram=[]\n",
    "    \n",
    "    for sentence in sentences_token:\n",
    "        m=0\n",
    "        while m+n <= len(sentence):\n",
    "            ngram.append(sentence[m:m+n])\n",
    "            m+=1\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['on', 'do', 'is'], ['do', 'is', 'be'], ['is', 'be', 'll'], ['be', 'll', 'mk'], ['opn', 'dpo', 'ips'], ['dpo', 'ips', 'bpe'], ['ips', 'bpe', 'lo']]\n"
     ]
    }
   ],
   "source": [
    "sentences_token=[['on',\n",
    " 'do',\n",
    " 'is',\n",
    " 'be','ll','mk'],['opn',\n",
    " 'dpo',\n",
    " 'ips',\n",
    " 'bpe','lo']]\n",
    "\n",
    "a = count_ngram(sentences_token, n=3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
